#!/usr/bin/env python
# coding: utf8

import logging
from logging.handlers import RotatingFileHandler
import uuid
import argparse
import subprocess
import progressbar
from urllib.parse import urlparse, urljoin
from furl import furl
import re
from requests.packages.urllib3.exceptions import InsecureRequestWarning
import requests
from termcolor import colored
from slugify import slugify
from datetime import datetime
from tqdm import tqdm
import urllib3
import codecs
import time
import sys
import os
import copy
from multiprocessing.dummy import Pool as ThreadPool
from libnmap.parser import NmapParser

valid_chars = "-_.()abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"

UA = "Mozilla/5.0 (X11; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0"
headers = dict()
headers["user-agent"] = UA
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

NB_THREADS = 20


def get_meta_redirect(resp):
	redirect_re = re.compile('<meta[^>]*?url=(.*?)["\']', re.IGNORECASE)
	match = redirect_re.search(resp.text)
	if match:
		url = urljoin(resp.url, match.groups()[0].strip())
	else:
		url = None
	return url


class Target(object):

	def __init__(self, scheme, host, port):
		self.host = host
		self.scheme = scheme
		self.port = port

	@property
	def url(self):
		return furl(scheme=self.scheme, host=self.host, port=self.port)

	def __str__(self):
		return self.url.url

	def __repr__(self):
		return str(self)

	def request(self, path):
		if path:
			r = requests.get(str(self.url / path), verify=False, timeout=5, headers=headers, allow_redirects=True)
		else:
			r = requests.get(str(self), verify=False, timeout=5, headers=headers, allow_redirects=True)
		return r


class HTTPResult(object):

	def __init__(self, response):
		self.response = response

	@property
	def code(self):
		return self.response.status_code

	@property
	def url(self):
		return self.response.url

	def __repr__(self):
		return "[%s] <%s>" % (self.response.status_code, self.url)

	def __str__(self):
		if self.response.is_redirect:
			s = requests.session()
			redirect = s.get_redirect_target(self.response)
			return "%s => %s (%s bytes)" % (self.url, redirect, len(self.response.text))
		else:
			return "%s (%s bytes)" % (self.url, len(self.response.text))

	def __eq__(self, other):
		return ((self.url == other.url) and (self.code == other.code))

	def __hash__(self):
		return (hash(self.url) ^ hash(self.code))


class ScrapReport(object):

	def __init__(self):
		self.values = dict()
		self.shown_codes = []
		self.hidden_codes = []

	def add(self, response):
		http_result = HTTPResult(response)
		if http_result.code not in self.values:
			self.values[http_result.code] = set()
		self.values[response.status_code].add(http_result)

	def show_code(self, code):
		self.shown_codes.append(code)

	def hide_code(self, code):
		self.hidden_codes.append(code)

	@property
	def urls(self):
		self._urls = set()
		for retcode, responses in self.values.iteritems():
			for response in responses:
				self._urls.add(response.url)
		return self._urls

	def report_by_code(self, code):
		if code in self.values.keys():
			title = "HTTP %s" % code
			output = title
			output += '\n'
			output += len(title) * '='
			output += '\n\n'
			for response in self.values[code]:
				output += "  * %s\n" % str(response)
			output += '\n'
			return output

	def __repr__(self):
		output = ''
		if self.shown_codes:
			for shown_code in self.shown_codes:
				output += self.report_by_code(shown_code)
		elif self.hidden_codes:
			for retcode in self.values.keys():
				if retcode not in self.hidden_codes:
					output += self.report_by_code(retcode)
		else:
			for retcode in self.values.keys():
				output += self.report_by_code(retcode)

		return output

	def save(self, suffix):
		with open("retcodes_" + suffix, 'w') as outfile:
			outfile.write(repr(self))

		with open("urls_" + suffix, 'w') as outfile:
			for url in self.urls:
				outfile.write("%s\n" % url)


class HTTPGrabber(object):

	def __init__(self, path):
		self.path = path

		self.report = ScrapReport()
		self.logger = logging.getLogger()
		self.logger.setLevel(logging.DEBUG)
		formatter = logging.Formatter("[%(levelname)s] %(asctime)s :: %(message)s")

		file_handler = RotatingFileHandler("http.log", 'a', 1000000, 1)
		file_handler.setLevel(logging.DEBUG)
		file_handler.setFormatter(formatter)
		self.logger.addHandler(file_handler)

	def check_meta(self, resp):
		meta = get_meta_redirect(resp)
		if meta:
			url = urlparse(meta)
			# <scheme>://<netloc>/<path>;<params>?<query>#<fragment>
			redir_path = url.path
			if url.params:
				redir_path += ";"
				redir_path += url.params
			if url.query:
				redir_path += "?"
				redir_path += url.query
			self.grab(Target(url.scheme, url.hostname, url.port), redir_path)

	def grab(self, target, path=None):
		self.logger.warn(target)

		try:
			if path is not None:
				response = target.request(path)
			else:
				response = target.request(self.path)
			for resp in response.history:
				self.report.add(resp)
				self.check_meta(resp)
			self.report.add(response)
			self.check_meta(response)
		except Exception:
			pass

#			if 'Error' not in results:
#				results['Error'] = []
#			results['Error'].append(str(target) + ' ' + type(e).__name__)


if __name__ == "__main__":
	parser = argparse.ArgumentParser("Nmap helper")
	subparsers = parser.add_subparsers(help="sub-command help", dest="subparser_name")

	# http subparser
	http_parser = subparsers.add_parser('http', help='Perform an HTTP analysis')
	http_parser.add_argument("--sc", help="HTTP status codes to show", required=False, default=None, type=int)
	http_parser.add_argument("--hc", help="HTTP status code to hide", required=False, default=None, type=int)
	http_parser.add_argument("--start", help="The start HTTP page", required=False, default=None)
	http_parser.add_argument("--screen", help="Takes a screenshot of each http service", action="store_true", required=False)
	http_parser.add_argument("--save", help="Persistent output", required=False, action="store_true")
	http_parser.add_argument("--threads", "-t", help="Number of threads to run", required=False, type=int, default=20)

	# port subparser
	port_parser = subparsers.add_parser('port', help='Returns a listing based on port state')
	port_parser.add_argument("--port", "-p", type=int, help="", required=False)
	port_parser.add_argument("--state", "-s", type=str, help="", required=False)

	parser.add_argument("xml_file", help="The nmap XML output", nargs='+')
	args = parser.parse_args()
	targets = []
	if args.subparser_name == "http":
		for nmap_file in args.xml_file:
			print("[%s] Parsing file %s" % (colored("*", "green"), nmap_file))
			nmap_report = NmapParser.parse_fromfile(nmap_file)
			start = args.start
			for host in nmap_report.hosts:
				for svc in host.services:
					if (svc.service == "http" or svc.service == "https") and svc.state == "open":
						if svc.tunnel == "ssl" or svc.service == "https":
							targets.append(Target("https", host.address, svc.port))
						else:
							targets.append(Target("http", host.address, svc.port))
						for hostname in host.hostnames:
							if hostname.endswith("."):
								hostname = hostname[:-1]
							if svc.tunnel == "ssl" or svc.service == "https":
								targets.append(Target("https", hostname, svc.port))
							else:
								targets.append(Target("http", hostname, svc.port))

			total = len(targets)
			httpgrabber = HTTPGrabber(args.start)

			pool = ThreadPool(args.threads)
			print("[%s] Found %d http services" % (colored('*', "green"), len(targets)))
			print("[%s] Launching %s threads" % (colored('!', "red"), args.threads))
			with tqdm(targets, ncols=50, bar_format="{percentage:1.0f}%| {bar} | {n_fmt}/{total_fmt}") as pbar:
				for _ in pool.imap_unordered(httpgrabber.grab, targets):
					pbar.update()
			pool.close()
			pool.join()

			if args.hc:
				httpgrabber.report.hide_code(args.hc)

			if args.sc:
				httpgrabber.report.show_code(args.sc)
			print(httpgrabber.report)

			if args.save:
				suffix = ''
				if args.start:
					suffix += slugify(start)
				suffix += slugify(str(datetime.now()))
				httpgrabber.report.save(suffix)

			if args.screen:
				if not os.path.isdir("screenshots"):
					os.mkdir("screenshots")

				for url in httpgrabber.report.urls:
					subprocess.call([os.path.dirname(__file__) + "/massws", "-s", "-o", "screenshots/%s.png" % "".join(c for c in url.replace("://", "_").replace("/", "_").replace(":", "_") if c in valid_chars), url])
	elif args.subparser_name == "port":
		for nmap_file in args.xml_file:
			nmap_report = NmapParser.parse_fromfile(nmap_file)
			for host in nmap_report.hosts:
				if args.port and not args.state:
					for svc in host.services:
						if svc.port == args.port:
							print(host.address)
				elif args.port and args.state:
					for svc in host.services:
						if svc.state == args.state and svc.port == args.port:
							print(host.address)
				else:
					print(host.address)
