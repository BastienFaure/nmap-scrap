#!/usr/bin/env python3
# coding: utf8

import logging
import shutil
from logging.handlers import RotatingFileHandler
import tempfile
import argparse
import subprocess
from urllib.parse import urlparse, urljoin
from furl import furl
import re
from requests.packages.urllib3.exceptions import InsecureRequestWarning
import requests
from termcolor import colored
from slugify import slugify
from datetime import datetime
from tqdm import tqdm
import urllib3
import os
from multiprocessing.dummy import Pool as ThreadPool
from libnmap.parser import NmapParser

valid_chars = "-_.()abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"

UA = "Mozilla/5.0 (X11; Linux x86_64; rv:47.0) Gecko/20100101 Firefox/47.0"
headers = dict()
headers["user-agent"] = UA
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

NB_THREADS = 20


def get_meta_redirect(resp):
	redirect_re = re.compile('<meta http-equiv[^>]*?url=[\'"]*([^>]*?)[\'"].*>', re.IGNORECASE)
	match = redirect_re.search(resp.text)
	url = urljoin(resp.url, match.groups()[0].strip())
	if match:
		url = urljoin(resp.url, match.groups()[0].strip())
		if url == resp.url:
			url = None
	else:
		url = None
	return url


class Target(object):

	def __init__(self, scheme, host, port):
		self.host = host
		self.scheme = scheme
		self.port = port

	@property
	def url(self):
		return furl(scheme=self.scheme, host=self.host, port=self.port)

	def __str__(self):
		return self.url.url

	def __repr__(self):
		return str(self)

	def __eq__(self, other):
		return str(self) == str(other)

	def __hash__(self):
		return hash(str(self))

	def request(self, path):
		if path:
			r = requests.get(str(self.url.join(path)), verify=False, timeout=5, headers=headers, allow_redirects=True)
		else:
			r = requests.get(str(self), verify=False, timeout=5, headers=headers, allow_redirects=True)
		return r


class HTTPResult(object):

	def __init__(self, response):
		self.response = response

	@property
	def code(self):
		return self.response.status_code

	@property
	def url(self):
		return furl(self.response.url).url

	def __repr__(self):
		return "[%s] <%s>" % (self.response.status_code, self.url)

	def __str__(self):
		if self.response.is_redirect:
			s = requests.session()
			try:
				redirect = furl(s.get_redirect_target(self.response)).url
			except Exception:
				redirect = s.get_redirect_target(self.response)
			return "%s => %s (%s bytes)" % (self.url, redirect, len(self.response.text))
		else:
			return "%s (%s bytes)" % (self.url, len(self.response.text))

	def __eq__(self, other):
		return ((self.url == other.url) and (self.code == other.code))

	def __hash__(self):
		return (hash(self.url) ^ hash(self.code))


class ScrapReport(object):

	def __init__(self):
		self.values = dict()
		self.shown_codes = []
		self.hidden_codes = []

	def add(self, response):
		http_result = HTTPResult(response)
		if http_result.code not in self.values:
			self.values[http_result.code] = set()
		self.values[response.status_code].add(http_result)

	def show_code(self, code):
		self.shown_codes.append(code)

	def hide_code(self, code):
		self.hidden_codes.append(code)

	@property
	def urls(self):
		self._urls = set()
		for retcode, responses in self.values.items():
			for response in responses:
				self._urls.add(response.url)
		return self._urls

	def report_by_code(self, code):
		if code in self.values.keys():
			title = "HTTP %s" % code
			output = title
			output += '\n'
			output += len(title) * '='
			output += '\n\n'
			for response in self.values[code]:
				output += "  * %s\n" % str(response)
				if code == 401:
					if "www-authenticate" in response.response.headers._store:
						auth_header = f"{response.response.headers['www-authenticate']}"
						auth_type = auth_header.strip().split(" ")[0].lower()
						if auth_type in ["basic", "digest"]:
							url = furl(response.url)
							output += f"\thydra -L logins.lst -P passwords.lst -t 1 -f -vV {url.host} -s {url.port} http-get {url.url}\n"
						else:
							output += f"\t{auth_header}\n"
			output += '\n'
			return output

	def __repr__(self):
		output = ''
		if self.shown_codes:
			for shown_code in self.shown_codes:
				output += self.report_by_code(shown_code)
		elif self.hidden_codes:
			for retcode in self.values.keys():
				if retcode not in self.hidden_codes:
					output += self.report_by_code(retcode)
		else:
			for retcode in self.values.keys():
				output += self.report_by_code(retcode)

		return output

	def save(self, suffix):
		self.retcodes_path = f"retcodes_{suffix}"
		self.urls_path = f"urls_{suffix}"

		with open(self.retcodes_path, 'w') as outfile:
			outfile.write(repr(self))

		with open(self.urls_path, 'w') as outfile:
			for url in self.urls:
				outfile.write("%s\n" % url)


class HTTPGrabber(object):

	def __init__(self, path):
		self.path = path

		self.report = ScrapReport()
		self.logger = logging.getLogger()
		self.logger.setLevel(logging.DEBUG)
		formatter = logging.Formatter("[%(levelname)s] %(asctime)s :: %(message)s")

		file_handler = RotatingFileHandler("http.log", 'a', 1000000, 1)
		file_handler.setLevel(logging.DEBUG)
		file_handler.setFormatter(formatter)
		self.logger.addHandler(file_handler)

	def check_meta(self, resp):
		meta = get_meta_redirect(resp)
		if meta:
			url = urlparse(meta)
			# <scheme>://<netloc>/<path>;<params>?<query>#<fragment>
			redir_path = url.path
			if url.params:
				redir_path += ";"
				redir_path += url.params
			if url.query:
				redir_path += "?"
				redir_path += url.query
			self.grab(Target(url.scheme, url.hostname, url.port), redir_path)

	def grab(self, target, path=None):
		self.logger.warning(target)

		try:
			if path is not None:
				response = target.request(path)
			else:
				response = target.request(self.path)
			for resp in response.history:
				self.report.add(resp)
				self.check_meta(resp)
			self.report.add(response)
			self.check_meta(response)
		except Exception:
			pass

#			if 'Error' not in results:
#				results['Error'] = []
#			results['Error'].append(str(target) + ' ' + type(e).__name__)


if __name__ == "__main__":
	parser = argparse.ArgumentParser("Nmap helper")
	subparsers = parser.add_subparsers(help="sub-command help", dest="subparser_name")

	# http subparser
	http_parser = subparsers.add_parser('http', help='Perform an HTTP analysis')
	http_parser.add_argument("--sc", help="HTTP status codes to show", required=False, default=None, type=int)
	http_parser.add_argument("--hc", help="HTTP status code to hide", required=False, default=None, type=int)
	http_parser.add_argument("--start", help="The start HTTP page", required=False, default=None)
	http_parser.add_argument("--screen", help="Takes a screenshot of each http service", action="store_true", required=False)
	http_parser.add_argument("--save", help="Persistent output", required=False, action="store_true")
	http_parser.add_argument("--threads", "-t", help="Number of threads to run", required=False, type=int, default=20)
	http_parser.add_argument("xml_file", help="The nmap XML output", nargs='+')

	# port subparser
	port_parser = subparsers.add_parser('port', help='Returns a listing based on port state')
	port_parser.add_argument("--port", "-p", type=int, help="", required=False)
	port_parser.add_argument("--state", "-s", type=str, help="", required=False)
	port_parser.add_argument("xml_file", help="The nmap XML output", nargs='+')

	args = parser.parse_args()
	targets = set()
	if args.subparser_name == "http":
		for nmap_file in args.xml_file:
			print("[%s] Parsing file %s" % (colored("*", "green"), nmap_file))
			nmap_report = NmapParser.parse_fromfile(nmap_file)
			start = args.start
			for host in nmap_report.hosts:
				for svc in host.services:
					if ("http" in svc.service) and svc.state == "open":
						for hostname in list(set([host.address] + list(set(map(lambda x: x.name, host.hostnames))))):
							if hostname.endswith('.'):
								hostname = hostname[:-1]
							if svc.tunnel == "ssl" or ("https" in svc.service):
								targets.add(Target("https", hostname, svc.port))
							else:
								targets.add(Target("http", hostname, svc.port))

		total = len(targets)
		httpgrabber = HTTPGrabber(args.start)

		pool = ThreadPool(args.threads)
		print("[%s] Found %d http services" % (colored('*', "green"), len(targets)))
		print("[%s] Launching %s threads" % (colored('!', "red"), args.threads))
		with tqdm(targets, ncols=50, bar_format="{percentage:1.0f}%| {bar} | {n_fmt}/{total_fmt}") as pbar:
			for _ in pool.imap_unordered(httpgrabber.grab, targets):
				pbar.update()
		pool.close()
		pool.join()

		if args.hc:
			httpgrabber.report.hide_code(args.hc)

		if args.sc:
			httpgrabber.report.show_code(args.sc)
		print(httpgrabber.report)

		if args.save:
			suffix = ''
			if args.start:
				suffix += slugify(start)
			suffix += slugify(str(datetime.now()))
			httpgrabber.report.save(suffix)

		if args.screen:
			gowitness_path = shutil.which("gowitness")
			if gowitness_path:
				with tempfile.NamedTemporaryFile(mode="w") as tmp:
					# write urls to temp file
					tmp.write("\n".join(httpgrabber.report.urls))
					tmp.flush()
					screens_cmd = ["gowitness", "file", "-f", tmp.name]
					subprocess.call(screens_cmd)
			else:
				print("[%s] gowitness is not installed or not available in the path, please check: https://github.com/sensepost/gowitness" % colored('!', "red"))


	elif args.subparser_name == "port":
		for nmap_file in args.xml_file:
			nmap_report = NmapParser.parse_fromfile(nmap_file)
			for host in nmap_report.hosts:
				if args.port and not args.state:
					for svc in host.services:
						if svc.port == args.port:
							print(host.address)
				elif args.port and args.state:
					for svc in host.services:
						if svc.state == args.state and svc.port == args.port:
							print(host.address)
				else:
					print(host.address)
